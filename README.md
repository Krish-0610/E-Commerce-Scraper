# Amazon & Flipkart Web Scraper

This project is a **web scraping application** designed to extract key product detailsâ€”including title, price, rating, and product linkâ€”from **Amazon** (with planned support for Flipkart). The system is developed using **Selenium** for automation, **Flask** for backend API integration, and a **frontend** built with HTML, CSS, and JavaScript.

---

## ğŸš€ **Current Project Status**
- âœ… Implemented a **Selenium-based scraper** for extracting product data.
- âœ… Developed a **Flask API** to connect the scraper with a web interface.
- âœ… Designed and integrated a **frontend (HTML, CSS, JavaScript)** for user interaction.
- âœ… Successfully tested data extraction from Amazon.
- âœ… Implemented **dynamic XPath handling** for more reliable element selection.
- âœ… Enhanced **error handling and performance optimization**.

---

## ğŸ“Œ **Key Features**
- ğŸ” Enables product search on **Amazon**.
- ğŸ“Š Extracts and returns **Title, Price, Rating, and Product URL**.
- âš¡ Provides a **frontend interface** for user interaction.
- ğŸŒ Utilizes a **Flask-based API** to handle backend requests.
- ğŸ”„ Supports **headless browsing** for optimized performance.
- ğŸ› ï¸ Designed for extensibility to **Flipkart and other e-commerce platforms**.

---

## ğŸ›  **Installation & Setup Guide**

### **1ï¸âƒ£ Clone the Repository**
```sh
git clone https://github.com/your-repo/web-scraper.git
cd web-scraper
```

### **2ï¸âƒ£ Install Required Dependencies**
Ensure **Python 3+** and **pip** are installed.

```sh
pip install -r requirements.txt
```
_(If `requirements.txt` is unavailable, refer to the dependencies list below.)_

### **3ï¸âƒ£ Launch the Backend Server**
```sh
cd backend
python app.py
```
The API will start on: `http://127.0.0.1:5000`

### **4ï¸âƒ£ Open the Frontend**
Simply open `frontend/index.html` in a browser.

---

## ğŸ›  **Dependencies**
The project relies on the following technologies:
- **Python 3.x** (Programming Language)
- **Flask** (Web API Framework)
- **Selenium** (Browser Automation for Scraping)
- **WebDriver Manager** (Automates WebDriver Installation)
- **Google Chrome** (Required for Selenium Execution)

### **Manual Dependency Installation**
If `requirements.txt` is unavailable, install dependencies manually:
```sh
pip install flask selenium webdriver-manager
```

---

## ğŸ”¥ **Usage Instructions**
1ï¸âƒ£ Open `frontend/index.html` in a web browser.  
2ï¸âƒ£ Enter a **product name** in the search field and submit the request.  
3ï¸âƒ£ The **Flask API** triggers the Selenium-based scraper.  
4ï¸âƒ£ Extracted product details are **displayed on the webpage**.  

---

## ğŸ“Œ **Planned Enhancements**
- ğŸ”„ **Extend support to Flipkart**.
- ğŸš€ **Deploy API to a cloud platform** (e.g., Heroku, Render, AWS).
- âš¡ **Optimize Selenium execution speed**.
- ğŸ›  **Enable multi-page scraping for more extensive data extraction**.
- ğŸ“Š **Implement CSV/JSON export functionality**.
- ğŸ” **Improve anti-bot detection handling**.

---

## ğŸ’¡ **Contributions & Contact**
Contributions are welcome! Feel free to fork the repository, report issues, or submit improvements.

ğŸ“§ Contact: kdp88532@gmail.com

